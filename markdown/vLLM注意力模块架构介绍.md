# vLLM æ³¨æ„åŠ›æ¨¡å—æ¶æ„ä»‹ç»

æœ¬æ–‡æ¡£ä»‹ç» vLLM ä¸­ä¸¤ä¸ªæ ¸å¿ƒæ³¨æ„åŠ›æ¨¡å—ï¼š`vllm/attention` å’Œ `vllm/v1/attention` çš„æ¶æ„è®¾è®¡å’ŒèŒè´£åˆ†å·¥ã€‚

---

## ç›®å½•
- [æ¦‚è¿°](#æ¦‚è¿°)
- [vllm/attention - ç»Ÿä¸€æ³¨æ„åŠ›æ¡†æ¶](#vllmattention---ç»Ÿä¸€æ³¨æ„åŠ›æ¡†æ¶)
- [vllm/v1/attention - V1 å¼•æ“ä¸“ç”¨åç«¯](#vllmv1attention---v1-å¼•æ“ä¸“ç”¨åç«¯)
- [ä¸¤è€…å…³ç³»ä¸åä½œ](#ä¸¤è€…å…³ç³»ä¸åä½œ)
- [ä½¿ç”¨å»ºè®®](#ä½¿ç”¨å»ºè®®)

---

## æ¦‚è¿°

vLLM 0.11.0 é‡‡ç”¨**åŒå±‚æ¶æ„**è®¾è®¡æ³¨æ„åŠ›æœºåˆ¶ï¼š

| æ¨¡å— | å®šä½ | èŒè´£ |
|------|------|------|
| **vllm/attention** | ç»Ÿä¸€æ³¨æ„åŠ›æ¡†æ¶å±‚ | æä¾›æŠ½è±¡æ¥å£ã€åç«¯é€‰æ‹©ã€å±‚å°è£… |
| **vllm/v1/attention** | V1 å¼•æ“æ‰§è¡Œå±‚ | å®ç°å…·ä½“çš„æ³¨æ„åŠ›è®¡ç®—å†…æ ¸ |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              vllm/attention                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  selector.py - åç«¯é€‰æ‹©å™¨                        â”‚ â”‚
â”‚  â”‚  layer.py - Attention ç±»ï¼ˆtorch.nn.Moduleï¼‰      â”‚ â”‚
â”‚  â”‚  backends/abstract.py - æŠ½è±¡æ¥å£                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ è°ƒç”¨
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           vllm/v1/attention/backends                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  flash_attn.py - Flash Attention å®ç°            â”‚ â”‚
â”‚  â”‚  flashinfer.py - FlashInfer å®ç°                 â”‚ â”‚
â”‚  â”‚  flex_attention.py - FlexAttention å®ç°          â”‚ â”‚
â”‚  â”‚  triton_attn.py - Triton Attention å®ç°          â”‚ â”‚
â”‚  â”‚  mla/ - Multi-head Latent Attention å®ç°         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## vllm/attention - ç»Ÿä¸€æ³¨æ„åŠ›æ¡†æ¶

### ğŸ“ ç›®å½•ç»“æ„

```
vllm/attention/
â”œâ”€â”€ __init__.py               # å¯¼å‡ºæ ¸å¿ƒæ¥å£
â”œâ”€â”€ selector.py               # åç«¯é€‰æ‹©é€»è¾‘
â”œâ”€â”€ layer.py                  # Attention å±‚å®ç°
â”œâ”€â”€ backends/                 # åç«¯æŠ½è±¡å±‚
â”‚   â”œâ”€â”€ abstract.py           # AttentionBackend æŠ½è±¡æ¥å£
â”‚   â””â”€â”€ utils.py              # åç«¯å·¥å…·å‡½æ•°
â”œâ”€â”€ layers/                   # ç‰¹æ®Šæ³¨æ„åŠ›å±‚
â”‚   â”œâ”€â”€ chunked_local_attention.py    # åˆ†å—å±€éƒ¨æ³¨æ„åŠ›
â”‚   â”œâ”€â”€ cross_attention.py            # äº¤å‰æ³¨æ„åŠ›
â”‚   â””â”€â”€ encoder_only_attention.py     # ç¼–ç å™¨ä¸“ç”¨æ³¨æ„åŠ›
â”œâ”€â”€ ops/                      # æ³¨æ„åŠ›æ“ä½œå®ç°
â”‚   â”œâ”€â”€ paged_attn.py         # åˆ†é¡µæ³¨æ„åŠ›
â”‚   â”œâ”€â”€ prefix_prefill.py     # å‰ç¼€é¢„å¡«å……
â”‚   â”œâ”€â”€ flashmla.py           # Flash MLA æ“ä½œ
â”‚   â”œâ”€â”€ merge_attn_states.py  # æ³¨æ„åŠ›çŠ¶æ€åˆå¹¶
â”‚   â””â”€â”€ ...                   # å…¶ä»–æ“ä½œ
â””â”€â”€ utils/                    # å·¥å…·å‡½æ•°
    â””â”€â”€ fa_utils.py           # Flash Attention å·¥å…·
```

### æ ¸å¿ƒèŒè´£

#### 1. **åç«¯é€‰æ‹©å™¨ (selector.py)**

è´Ÿè´£æ ¹æ®ç¯å¢ƒå˜é‡ã€ç¡¬ä»¶å¹³å°ã€æ¨¡å‹é…ç½®é€‰æ‹©æœ€åˆé€‚çš„æ³¨æ„åŠ›åç«¯ã€‚

**å…³é”®å‡½æ•°**ï¼š
- `backend_name_to_enum(backend_name: str)` - å­—ç¬¦ä¸²è½¬æšä¸¾
- `get_env_variable_attn_backend()` - è¯»å–ç¯å¢ƒå˜é‡
- `get_attn_backend()` - è¿”å›é€‰ä¸­çš„åç«¯ç±»

**é€‰æ‹©ä¼˜å…ˆçº§**ï¼š
```python
1. å…¨å±€å¼ºåˆ¶æŒ‡å®š (force_attn_backend_ctx_manager)
2. ç¯å¢ƒå˜é‡æŒ‡å®š (VLLM_ATTENTION_BACKEND)
3. å¹³å°è‡ªåŠ¨é€‰æ‹© (current_platform.get_attn_backend_cls)
```

#### 2. **æ³¨æ„åŠ›å±‚ (layer.py)**

æä¾› `Attention` ç±»ä½œä¸º `torch.nn.Module`ï¼Œå°è£…æ³¨æ„åŠ›è®¡ç®—ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
```python
class Attention(nn.Module, AttentionLayerBase):
    """
    æ³¨æ„åŠ›å±‚å®ç°ï¼š
    1. å­˜å‚¨ key/value åˆ° KV cache
    2. æ‰§è¡Œå¤šå¤´/å¤šæŸ¥è¯¢/åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›
    3. è¿”å›æ³¨æ„åŠ›è¾“å‡ºå¼ é‡
    """
    def __init__(
        self,
        num_heads: int,
        head_size: int,
        scale: float,
        num_kv_heads: Optional[int] = None,
        alibi_slopes: Optional[List[float]] = None,
        cache_config: Optional[CacheConfig] = None,
        use_mla: bool = False,  # Multi-head Latent Attention
        attn_type: str = AttentionType.DECODER,
        ...
    )
```

**æ”¯æŒçš„æ³¨æ„åŠ›ç±»å‹**ï¼š
- `DECODER` - è§£ç å™¨è‡ªæ³¨æ„åŠ›
- `ENCODER` - ç¼–ç å™¨è‡ªæ³¨æ„åŠ›ï¼ˆç¼–ç å™¨-è§£ç å™¨ï¼‰
- `ENCODER_ONLY` - çº¯ç¼–ç å™¨è‡ªæ³¨æ„åŠ›
- `ENCODER_DECODER` - ç¼–ç å™¨-è§£ç å™¨äº¤å‰æ³¨æ„åŠ›

#### 3. **æŠ½è±¡æ¥å£ (backends/abstract.py)**

å®šä¹‰æ‰€æœ‰æ³¨æ„åŠ›åç«¯å¿…é¡»å®ç°çš„æ¥å£ã€‚

**æ ¸å¿ƒæŠ½è±¡ç±»**ï¼š
```python
class AttentionBackend(ABC):
    """æ‰€æœ‰æ³¨æ„åŠ›åç«¯çš„åŸºç±»"""
    
    accept_output_buffer: bool = False
    supports_quant_query_input: bool = False
    
    @staticmethod
    @abstractmethod
    def get_name() -> str:
        """è¿”å›åç«¯åç§°ï¼ˆå¦‚ 'FLASH_ATTN'ï¼‰"""
    
    @staticmethod
    @abstractmethod
    def get_impl_cls() -> Type["AttentionImpl"]:
        """è¿”å›å…·ä½“å®ç°ç±»"""
    
    @staticmethod
    @abstractmethod
    def get_metadata_cls() -> Type["AttentionMetadata"]:
        """è¿”å›å…ƒæ•°æ®ç±»"""
    
    @staticmethod
    @abstractmethod
    def get_kv_cache_shape(...) -> Tuple[int, ...]:
        """è¿”å› KV cache å½¢çŠ¶"""
```

#### 4. **é«˜çº§æ³¨æ„åŠ›å±‚ (layers/)**

æä¾›ç‰¹æ®Šåœºæ™¯çš„æ³¨æ„åŠ›å®ç°ï¼š

- **chunked_local_attention.py** - åˆ†å—å±€éƒ¨æ³¨æ„åŠ›ï¼Œé€‚ç”¨äºé•¿åºåˆ—
- **cross_attention.py** - äº¤å‰æ³¨æ„åŠ›ï¼Œç”¨äºç¼–ç å™¨-è§£ç å™¨æ¶æ„
- **encoder_only_attention.py** - çº¯ç¼–ç å™¨æ³¨æ„åŠ›ï¼ˆBERT ç±»æ¨¡å‹ï¼‰

#### 5. **æ³¨æ„åŠ›æ“ä½œ (ops/)**

åº•å±‚æ³¨æ„åŠ›æ“ä½œçš„é›†åˆï¼š

- **paged_attn.py** - PagedAttentionï¼Œæ”¯æŒä¸è¿ç»­å†…å­˜å—
- **prefix_prefill.py** - å‰ç¼€ç¼“å­˜é¢„å¡«å……
- **flashmla.py** - Flash MLA æ“ä½œï¼ˆDeepSeek-V2 ç­‰ï¼‰
- **merge_attn_states.py** - åˆå¹¶å¤šä¸ªæ³¨æ„åŠ›çŠ¶æ€
- **triton_flash_attention.py** - Triton å®ç°çš„ Flash Attention

**å…³é”®æ“ä½œç¤ºä¾‹**ï¼š
```python
# PagedAttention å‡½æ•°ç­¾å
def paged_attention_v1(
    query: torch.Tensor,          # [num_seqs, num_heads, head_size]
    key_cache: torch.Tensor,      # [num_blocks, block_size, num_kv_heads, head_size]
    value_cache: torch.Tensor,    # [num_blocks, block_size, num_kv_heads, head_size]
    block_tables: torch.Tensor,   # [num_seqs, max_num_blocks_per_seq]
    context_lens: torch.Tensor,   # [num_seqs]
    ...
) -> torch.Tensor:
```

---

## vllm/v1/attention - V1 å¼•æ“ä¸“ç”¨åç«¯

### ğŸ“ ç›®å½•ç»“æ„

```
vllm/v1/attention/
â”œâ”€â”€ __init__.py               # ç©ºæ–‡ä»¶ï¼ˆåç«¯ç”± vllm/attention å¯¼å…¥ä½¿ç”¨ï¼‰
â””â”€â”€ backends/                 # V1 å¼•æ“ä¸“ç”¨å®ç°
    â”œâ”€â”€ flash_attn.py         # Flash Attention V1 å®ç°
    â”œâ”€â”€ flashinfer.py         # FlashInfer V1 å®ç°
    â”œâ”€â”€ flex_attention.py     # FlexAttention V1 å®ç°
    â”œâ”€â”€ triton_attn.py        # Triton Attention V1 å®ç°
    â”œâ”€â”€ xformers.py           # XFormers V1 å®ç°
    â”œâ”€â”€ tree_attn.py          # TreeAttention V1 å®ç°
    â”œâ”€â”€ cpu_attn.py           # CPU æ³¨æ„åŠ›å®ç°
    â”œâ”€â”€ rocm_attn.py          # ROCm/AMD GPU å®ç°
    â”œâ”€â”€ pallas.py             # Google Pallas/TPU å®ç°
    â”œâ”€â”€ gdn_attn.py           # GDN Attention å®ç°
    â”œâ”€â”€ linear_attn.py        # Linear Attention å®ç°
    â”œâ”€â”€ short_conv_attn.py    # çŸ­å·ç§¯æ³¨æ„åŠ›
    â”œâ”€â”€ mamba*.py             # Mamba/SSM çŠ¶æ€ç©ºé—´æ¨¡å‹
    â”œâ”€â”€ mla/                  # Multi-head Latent Attention ç›®å½•
    â”‚   â”œâ”€â”€ common.py         # MLA å…¬å…±ä»£ç 
    â”‚   â”œâ”€â”€ flashmla.py       # Flash MLA å®ç°
    â”‚   â”œâ”€â”€ cutlass_mla.py    # CUTLASS MLA å®ç°
    â”‚   â”œâ”€â”€ flashinfer_mla.py # FlashInfer MLA å®ç°
    â”‚   â”œâ”€â”€ flashattn_mla.py  # Flash Attention MLA å®ç°
    â”‚   â”œâ”€â”€ triton_mla.py     # Triton MLA å®ç°
    â”‚   â””â”€â”€ indexer.py        # MLA ç´¢å¼•å™¨
    â””â”€â”€ utils.py              # V1 åç«¯å·¥å…·å‡½æ•°
```

### æ ¸å¿ƒèŒè´£

#### 1. **Flash Attention V1 åç«¯ (flash_attn.py)**

åŸºäº vLLM å†…ç½®çš„ Flash Attention å®ç°ï¼ˆä» vLLM fork ç¼–è¯‘ï¼‰ã€‚

**å…³é”®ç‰¹æ€§**ï¼š
```python
class FlashAttentionBackend(AttentionBackend):
    accept_output_buffer: bool = True
    supports_quant_query_input: bool = True
    
    @classmethod
    def get_supported_dtypes(cls) -> list[torch.dtype]:
        return [torch.float16, torch.bfloat16]
    
    @classmethod
    def get_supported_head_sizes(cls) -> list[int]:
        return [32, 64, 96, 128, 160, 192, 224, 256]
    
    @staticmethod
    def get_kv_cache_shape(...) -> tuple[int, ...]:
        # è¿”å› (2, num_blocks, block_size, num_kv_heads, head_size)
        return (2, num_blocks, block_size, num_kv_heads, head_size)
```

**é€‚ç”¨åœºæ™¯**ï¼š
- SM 8.0+ NVIDIA GPUï¼ˆA100, A6000, RTX 3090, RTX 4090 ç­‰ï¼‰
- FP16/BF16 ç²¾åº¦æ¨ç†
- Head size â‰¤ 256

#### 2. **FlashInfer V1 åç«¯ (flashinfer.py)**

ä½¿ç”¨ç‹¬ç«‹çš„ FlashInfer åº“ï¼ˆéœ€é¢å¤–å®‰è£…ï¼‰ã€‚

**å…³é”®ç‰¹æ€§**ï¼š
```python
class FlashInferBackend(AttentionBackend):
    """
    ä½¿ç”¨ FlashInfer å®ç°ï¼š
    - BatchDecodeWithPagedKVCacheWrapperï¼ˆè§£ç é˜¶æ®µï¼‰
    - BatchPrefillWithPagedKVCacheWrapperï¼ˆé¢„å¡«å……é˜¶æ®µï¼‰
    - MultiLevelCascadeAttentionWrapperï¼ˆçº§è”æ³¨æ„åŠ›ï¼‰
    """
    
    # æ”¯æŒ TRT-LLM åŠ é€Ÿè·¯å¾„
    supports_trtllm_attention: bool = True
    
    # æ”¯æŒ FP8 KV cache
    supports_fp8_kv_cache: bool = True
```

**é€‚ç”¨åœºæ™¯**ï¼š
- SM 10.0 (Blackwell B200/B100) ä¼˜å…ˆ
- SM 8.0-9.0 ä¹Ÿæ”¯æŒ
- éœ€è¦é«˜æ€§èƒ½åˆ†é¡µæ³¨æ„åŠ›
- FP8 KV cache é‡åŒ–

**å®‰è£…æ–¹å¼**ï¼š
```bash
pip install flashinfer -i https://flashinfer.ai/whl/cu124/torch2.4/
```

#### 3. **FlexAttention V1 åç«¯ (flex_attention.py)**

ä½¿ç”¨ PyTorch å†…ç½®çš„ FlexAttention APIï¼ˆtorch.nn.attention.flex_attentionï¼‰ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- SM < 8.0 çš„æ—§ GPUï¼ˆP100, V100 ç­‰ï¼‰
- ä¸æ”¯æŒ Flash Attention çš„ç¡¬ä»¶
- ä»»æ„ head size
- å…¼å®¹æ€§åå¤‡æ–¹æ¡ˆ

#### 4. **MLA ä¸“ç”¨å®ç° (mla/ ç›®å½•)**

Multi-head Latent Attentionï¼ˆå¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼‰çš„ä¸“é—¨ä¼˜åŒ–ï¼Œç”¨äº DeepSeek-V2ã€DeepSeek-V3 ç­‰æ¨¡å‹ã€‚

**æ”¯æŒçš„ MLA åç«¯**ï¼š

| åç«¯ | æ–‡ä»¶ | é€‚ç”¨ GPU | Block Size |
|------|------|----------|------------|
| **FlashMLA** | `flashmla.py` | SM 9.0 (H100, H200) | 64 |
| **CUTLASS MLA** | `cutlass_mla.py` | SM 10.0 (B200, B100) | 128 |
| **FlashInfer MLA** | `flashinfer_mla.py` | SM 10.0 | 32/64 |
| **FlashAttn MLA** | `flashattn_mla.py` | SM 8.0+ | é€šç”¨ |
| **Triton MLA** | `triton_mla.py` | æ‰€æœ‰ GPU | é€šç”¨ |

**MLA ç´¢å¼•å™¨ (indexer.py)**ï¼š
```python
class MLAIndexer:
    """
    ç®¡ç† MLA çš„å‹ç¼© KV å­˜å‚¨ï¼š
    - kv_lora_rank: å‹ç¼©ç»´åº¦ï¼ˆå¦‚ 512ï¼‰
    - qk_rope_head_dim: RoPE æ—‹è½¬ä½ç½®ç¼–ç ç»´åº¦
    - v_head_dim: Value æŠ•å½±ç»´åº¦
    """
```

#### 5. **ç‰¹æ®Šæ¶æ„æ”¯æŒ**

- **Mamba/SSM** (`mamba*.py`) - çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆé Transformer æ³¨æ„åŠ›ï¼‰
- **Linear Attention** (`linear_attn.py`) - çº¿æ€§å¤æ‚åº¦æ³¨æ„åŠ›
- **Short Conv Attention** (`short_conv_attn.py`) - çŸ­å·ç§¯æ³¨æ„åŠ›
- **GDN Attention** (`gdn_attn.py`) - Gated Depthwise Network æ³¨æ„åŠ›

#### 6. **å¹³å°ç‰¹å®šå®ç°**

- **CPU** (`cpu_attn.py`) - CPU ä¼˜åŒ–å®ç°ï¼ˆç”¨äºè°ƒè¯•æˆ–çº¯ CPU æ¨ç†ï¼‰
- **ROCm** (`rocm_attn.py`, `rocm_aiter_fa.py`) - AMD GPU å®ç°
- **TPU** (`pallas.py`) - Google TPU ä½¿ç”¨ Pallas ç¼–è¯‘å™¨

---

## ä¸¤è€…å…³ç³»ä¸åä½œ

### è°ƒç”¨æµç¨‹

```
ç”¨æˆ·ä»£ç 
   â”‚
   â”œâ”€ å¯¼å…¥: from vllm import LLM
   â”‚
   â”œâ”€ åˆå§‹åŒ–: llm = LLM(model="...")
   â”‚
   â””â”€> vllm/attention/layer.py
           â”‚
           â”œâ”€ Attention.__init__()
           â”‚   â””â”€> vllm/attention/selector.py
           â”‚       â””â”€ get_attn_backend()
           â”‚           â”œâ”€ 1. è¯»å– VLLM_ATTENTION_BACKEND ç¯å¢ƒå˜é‡
           â”‚           â”œâ”€ 2. è°ƒç”¨ backend_name_to_enum("FLASHINFER")
           â”‚           â””â”€ 3. è¿”å› FlashInferBackend ç±»
           â”‚
           â””â”€ Attention.forward()
               â””â”€> vllm/v1/attention/backends/flashinfer.py
                   â””â”€ FlashInferImpl.forward()
                       â””â”€ flashinfer.BatchPrefillWithPagedKVCacheWrapper.run()
```

### å…³é”®æ¥å£å¥‘çº¦

`vllm/attention` å®šä¹‰æ¥å£ï¼Œ`vllm/v1/attention` æä¾›å®ç°ï¼š

```python
# vllm/attention/backends/abstract.py
class AttentionBackend(ABC):
    @staticmethod
    @abstractmethod
    def get_impl_cls() -> Type["AttentionImpl"]:
        """è¿”å›å…·ä½“å®ç°ç±»"""

# vllm/v1/attention/backends/flash_attn.py
class FlashAttentionBackend(AttentionBackend):
    @staticmethod
    def get_impl_cls() -> type["FlashAttentionImpl"]:
        return FlashAttentionImpl  # å…·ä½“å®ç°
```

### æ•°æ®æµ

```
è¾“å…¥å¼ é‡
   â”‚
   â–¼
vllm/attention/layer.py (Attention.forward)
   â”‚
   â”œâ”€ Q/K/V æŠ•å½±
   â”œâ”€ é‡åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
   â”‚
   â–¼
vllm/v1/attention/backends/*.py
   â”‚
   â”œâ”€ KV cache å†™å…¥
   â”œâ”€ æ³¨æ„åŠ›è®¡ç®—
   â”‚   â”œâ”€ Prefill: flash_attn_varlen_func()
   â”‚   â””â”€ Decode: paged_decode_kernel()
   â”‚
   â–¼
è¾“å‡ºå¼ é‡
```

### å…±äº«ç»„ä»¶

| ç»„ä»¶ | ä½ç½® | ä½œç”¨ |
|------|------|------|
| **AttentionMetadata** | `vllm/attention/backends/abstract.py` | å…ƒæ•°æ®åŸºç±» |
| **AttentionType** | `vllm/attention/backends/abstract.py` | æ³¨æ„åŠ›ç±»å‹æšä¸¾ |
| **merge_attn_states** | `vllm/attention/ops/merge_attn_states.py` | åˆå¹¶æ³¨æ„åŠ›çŠ¶æ€ |
| **reshape_and_cache_flash** | `vllm/attention/utils/fa_utils.py` | Flash Attention KV cache Reshape |

---

## ä½¿ç”¨å»ºè®®

### 1. é€‰æ‹©åˆé€‚çš„åç«¯

| ç¡¬ä»¶å¹³å° | æ¨èåç«¯ | é…ç½®æ–¹æ³• |
|---------|---------|---------|
| **RTX 4090 / SM 8.9** | FlashInfer | `export VLLM_ATTENTION_BACKEND=FLASHINFER` |
| **A100 / SM 8.0** | Flash Attention | é»˜è®¤ï¼ˆæ— éœ€é…ç½®ï¼‰ |
| **H100 / SM 9.0** | FlashInfer | `export VLLM_ATTENTION_BACKEND=FLASHINFER` |
| **B200 / SM 10.0** | FlashInfer | é»˜è®¤ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰ |
| **V100 / SM 7.0** | FlexAttention | `export VLLM_ATTENTION_BACKEND=FLEX_ATTENTION` |
| **AMD ROCm** | ROCm Attention | è‡ªåŠ¨æ£€æµ‹ |
| **Google TPU** | Pallas | è‡ªåŠ¨æ£€æµ‹ |
| **CPU** | CPU Attention | è‡ªåŠ¨æ£€æµ‹ |

### 2. MLA æ¨¡å‹æ¨èé…ç½®

å¦‚æœä½¿ç”¨ DeepSeek-V2/V3 ç­‰ MLA æ¶æ„æ¨¡å‹ï¼š

```bash
# H100 GPUï¼ˆSM 9.0ï¼‰
export VLLM_ATTENTION_BACKEND=FLASHMLA

# B200 GPUï¼ˆSM 10.0ï¼‰
export VLLM_ATTENTION_BACKEND=CUTLASS_MLA

# é€šç”¨é…ç½®ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰
export VLLM_ATTENTION_BACKEND=FLASH_ATTN_MLA
```

### 3. è°ƒè¯•å»ºè®®

**æŸ¥çœ‹å®é™…ä½¿ç”¨çš„åç«¯**ï¼š
```python
import logging
logging.basicConfig(level=logging.INFO)

from vllm import LLM
llm = LLM(model="meta-llama/Llama-2-7b-hf")
# æ—¥å¿—è¾“å‡º: INFO cuda.py:313] Using FlashInfer backend on V1 engine.
```

**éªŒè¯åç«¯åŠŸèƒ½**ï¼š
```python
from vllm.attention.selector import get_attn_backend
from vllm.platforms import _Backend

backend = get_attn_backend(head_size=128, dtype=torch.float16, kv_cache_dtype="auto")
print(f"Selected backend: {backend.get_name()}")
print(f"Supports FP8 KV cache: {hasattr(backend, 'supports_fp8_kv_cache')}")
```

### 4. æ€§èƒ½ä¼˜åŒ–å»ºè®®

- **å¯ç”¨ FP8 KV cache**ï¼ˆFlashInfer åç«¯ï¼‰ï¼šå‡å°‘æ˜¾å­˜å ç”¨
  ```bash
  export VLLM_ATTENTION_BACKEND=FLASHINFER
  # KV cache é‡åŒ–åœ¨å¯åŠ¨æ—¶è‡ªåŠ¨å¯ç”¨ï¼ˆå¦‚æœç¡¬ä»¶æ”¯æŒï¼‰
  ```

- **ä½¿ç”¨ CUDA Graph**ï¼šå‡å°‘ kernel å¯åŠ¨å¼€é”€
  ```python
  llm = LLM(
      model="meta-llama/Llama-2-7b-hf",
      enforce_eager=False,  # å¯ç”¨ CUDA Graphï¼ˆé»˜è®¤ï¼‰
  )
  ```

- **è°ƒæ•´ block_size**ï¼š
  ```python
  llm = LLM(
      model="meta-llama/Llama-2-7b-hf",
      block_size=16,  # é»˜è®¤å€¼ï¼Œå¯è°ƒæ•´ä¸º 32/64ï¼ˆå–å†³äºåç«¯ï¼‰
  )
  ```

---

## å‚è€ƒä»£ç ä½ç½®

| åŠŸèƒ½ | æ–‡ä»¶è·¯å¾„ |
|------|---------|
| åç«¯é€‰æ‹©é€»è¾‘ | [vllm/attention/selector.py](../vllm/attention/selector.py) |
| Attention å±‚ | [vllm/attention/layer.py](../vllm/attention/layer.py) |
| æŠ½è±¡æ¥å£ | [vllm/attention/backends/abstract.py](../vllm/attention/backends/abstract.py) |
| Flash Attention V1 | [vllm/v1/attention/backends/flash_attn.py](../vllm/v1/attention/backends/flash_attn.py) |
| FlashInfer V1 | [vllm/v1/attention/backends/flashinfer.py](../vllm/v1/attention/backends/flashinfer.py) |
| MLA å®ç° | [vllm/v1/attention/backends/mla/](../vllm/v1/attention/backends/mla/) |
| PagedAttention æ“ä½œ | [vllm/attention/ops/paged_attn.py](../vllm/attention/ops/paged_attn.py) |

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**æ›´æ–°æ—¥æœŸ**ï¼š2026 å¹´ 2 æœˆ 14 æ—¥  
**é€‚ç”¨ vLLM ç‰ˆæœ¬**ï¼š0.11.0+  
**ç»´æŠ¤è€…**ï¼švLLM Community
