# vLLM API Server å¯åŠ¨æµç¨‹è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æ `python -m vllm.entrypoints.openai.api_server` å‘½ä»¤çš„å®Œæ•´æ‰§è¡Œæµç¨‹ã€‚

---

## ç›®å½•
- [å‘½ä»¤ç¤ºä¾‹](#å‘½ä»¤ç¤ºä¾‹)
- [æ—¥å¿—è¾“å‡ºåˆ†æ](#æ—¥å¿—è¾“å‡ºåˆ†æ)
- [å®Œæ•´æ‰§è¡Œæµç¨‹](#å®Œæ•´æ‰§è¡Œæµç¨‹)
- [æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–](#æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–)
- [æ—¶é—´çº¿åˆ†æ](#æ—¶é—´çº¿åˆ†æ)
- [å…³é”®ä»£ç è·¯å¾„](#å…³é”®ä»£ç è·¯å¾„)

---

## å‘½ä»¤ç¤ºä¾‹

```bash
CUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.openai.api_server \
    --model /data/ysh/models/Llama-3.1-8B-Instruct/ \
    --served-model-name meta-llama/Llama-3.1-8B-Instruct \
    --host 0.0.0.0 \
    --port 6578 \
    --gpu-memory-utilization 0.90 \
    --max-model-len 10000
```

---

## æ—¥å¿—è¾“å‡ºåˆ†æ

æ ¹æ®æ—¥å¿—è¾“å‡ºï¼Œå¯åŠ¨è¿‡ç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š

| é˜¶æ®µ | æ—¶é—´æˆ³ | å…³é”®æ—¥å¿— | è€—æ—¶ |
|------|--------|---------|------|
| **1. æœåŠ¡å™¨åˆå§‹åŒ–** | 16:10:22 | `vLLM API server version 0.1.dev9966` | - |
| **2. å‚æ•°è§£æ** | 16:10:22 | `non-default args: {...}` | < 1s |
| **3. æ¨¡å‹é…ç½®åŠ è½½** | 16:10:22 | `Resolved architecture: LlamaForCausalLM` | < 1s |
| **4. å¹³å°æ£€æµ‹** | 16:10:25 | `Automatically detected platform cuda` | 3s |
| **5. å¼•æ“æ ¸å¿ƒåˆå§‹åŒ–** | 16:10:27 | `Initializing a V1 LLM engine` | 2s |
| **6. æ¨¡å‹åŠ è½½** | 16:10:27-33 | `Loading safetensors checkpoint shards` | 6s |
| **7. torch.compile** | 16:10:38-11:04 | `Compiling a graph for dynamic shape` | 26s |
| **8. KV cache åˆ†é…** | 16:11:05 | `GPU KV cache size: 40,528 tokens` | 1s |
| **9. CUDA Graph æ•è·** | 16:11:05-11 | `Capturing CUDA graphs` | 6s |
| **10. æœåŠ¡å¯åŠ¨** | 16:11:12 | `Starting vLLM API server 0 on http://0.0.0.0:6578` | 1s |

**æ€»è€—æ—¶**ï¼šçº¦ 50 ç§’ï¼ˆä»å¯åŠ¨åˆ°æœåŠ¡å°±ç»ªï¼‰

---

## å®Œæ•´æ‰§è¡Œæµç¨‹

### æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å‘½ä»¤è¡Œå…¥å£                                    â”‚
â”‚  python -m vllm.entrypoints.openai.api_server [args]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. __main__ å…¥å£ (api_server.py:1942)                         â”‚
â”‚     - cli_env_setup()           # CLI ç¯å¢ƒè®¾ç½®                  â”‚
â”‚     - make_arg_parser(parser)   # åˆ›å»ºå‚æ•°è§£æå™¨                â”‚
â”‚     - parser.parse_args()       # è§£æå‘½ä»¤è¡Œå‚æ•°                â”‚
â”‚     - validate_parsed_serve_args(args)  # éªŒè¯å‚æ•°              â”‚
â”‚     - uvloop.run(run_server(args))      # å¯åŠ¨å¼‚æ­¥æœåŠ¡å™¨        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. run_server() (api_server.py:1877)                          â”‚
â”‚     - decorate_logs("APIServer")  # æ·»åŠ æ—¥å¿—å‰ç¼€                â”‚
â”‚     - setup_server(args)          # è®¾ç½®ç›‘å¬åœ°å€å’Œ socket      â”‚
â”‚     - run_server_worker(...)      # å¯åŠ¨å·¥ä½œè¿›ç¨‹                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. run_server_worker() (api_server.py:1887)                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚     â”‚ async with build_async_engine_client(args):          â”‚  â”‚
â”‚     â”‚   # å¼•æ“å®¢æˆ·ç«¯ç”Ÿå‘½å‘¨æœŸç®¡ç†                             â”‚  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. build_async_engine_client() (api_server.py:152)            â”‚
â”‚     - AsyncEngineArgs.from_cli_args(args)  # è½¬æ¢ä¸ºå¼•æ“å‚æ•°    â”‚
â”‚     - build_async_engine_client_from_engine_args(...)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. build_async_engine_client_from_engine_args()               â”‚
â”‚     (api_server.py:190)                                         â”‚
â”‚                                                                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆ›å»º VllmConfig                                â”‚       â”‚
â”‚     â”‚ vllm_config = engine_args.create_engine_config() â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆ›å»º V1 AsyncLLM å¼•æ“                          â”‚       â”‚
â”‚     â”‚ async_llm = AsyncLLM.from_vllm_config(          â”‚       â”‚
â”‚     â”‚     vllm_config=vllm_config,                    â”‚       â”‚
â”‚     â”‚     usage_context=...,                          â”‚       â”‚
â”‚     â”‚     enable_log_requests=...,                    â”‚       â”‚
â”‚     â”‚     disable_log_stats=...,                      â”‚       â”‚
â”‚     â”‚     client_count=...,                           â”‚       â”‚
â”‚     â”‚     client_index=...                            â”‚       â”‚
â”‚     â”‚ )                                               â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # æ¸…ç†å¤šæ¨¡æ€ç¼“å­˜                                 â”‚       â”‚
â”‚     â”‚ await async_llm.reset_mm_cache()                â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ yield async_llm  # è¿”å›å¼•æ“å®¢æˆ·ç«¯               â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. AsyncLLM.__init__() (v1/engine/async_llm.py:54)            â”‚
â”‚                                                                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆå§‹åŒ– Tokenizer                              â”‚       â”‚
â”‚     â”‚ self.tokenizer = init_tokenizer_from_configs(   â”‚       â”‚
â”‚     â”‚     model_config=vllm_config.model_config       â”‚       â”‚
â”‚     â”‚ )                                               â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆ›å»ºè¾“å…¥å¤„ç†å™¨                                 â”‚       â”‚
â”‚     â”‚ self.processor = Processor(                     â”‚       â”‚
â”‚     â”‚     vllm_config=vllm_config,                    â”‚       â”‚
â”‚     â”‚     tokenizer=self.tokenizer,                   â”‚       â”‚
â”‚     â”‚     mm_registry=mm_registry                     â”‚       â”‚
â”‚     â”‚ )                                               â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆ›å»ºè¾“å‡ºå¤„ç†å™¨                                 â”‚       â”‚
â”‚     â”‚ self.output_processor = OutputProcessor(        â”‚       â”‚
â”‚     â”‚     self.tokenizer,                             â”‚       â”‚
â”‚     â”‚     log_stats=self.log_stats                    â”‚       â”‚
â”‚     â”‚ )                                               â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆ›å»ºå¼•æ“æ ¸å¿ƒå®¢æˆ·ç«¯ï¼ˆå¤šè¿›ç¨‹ï¼‰                    â”‚       â”‚
â”‚     â”‚ self.engine_core = EngineCoreClient.           â”‚       â”‚
â”‚     â”‚     make_async_mp_client(                       â”‚       â”‚
â”‚     â”‚         vllm_config=vllm_config,                â”‚       â”‚
â”‚     â”‚         executor_class=executor_class,          â”‚       â”‚
â”‚     â”‚         log_stats=self.log_stats,               â”‚       â”‚
â”‚     â”‚         client_count=client_count,              â”‚       â”‚
â”‚     â”‚         client_index=client_index               â”‚       â”‚
â”‚     â”‚     )                                           â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆ›å»ºç»Ÿè®¡æ—¥å¿—ç®¡ç†å™¨                             â”‚       â”‚
â”‚     â”‚ self.logger_manager = StatLoggerManager(...)    â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # å¯åŠ¨è¾“å‡ºå¤„ç†åç¨‹                               â”‚       â”‚
â”‚     â”‚ self._run_output_handler()                      â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. EngineCoreClient.make_async_mp_client()                     â”‚
â”‚     (v1/engine/core_client.py:85)                               â”‚
â”‚                                                                 â”‚
â”‚     åˆ›å»ºå¤šè¿›ç¨‹å¼•æ“æ ¸å¿ƒï¼š                                         â”‚
â”‚     - å¯åŠ¨ç‹¬ç«‹çš„ EngineCore è¿›ç¨‹ï¼ˆEngineCore_DP0ï¼‰              â”‚
â”‚     - é€šè¿‡ ZMQ socket è¿›è¡Œ IPC é€šä¿¡                             â”‚
â”‚     - å‘é€åˆå§‹åŒ–æ¶ˆæ¯åˆ°å¼•æ“æ ¸å¿ƒ                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. EngineCore.__init__() (v1/engine/core.py:62)               â”‚
â”‚     [åœ¨ç‹¬ç«‹è¿›ç¨‹ EngineCore_DP0 ä¸­è¿è¡Œ]                          â”‚
â”‚                                                                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åŠ è½½æ’ä»¶                                       â”‚       â”‚
â”‚     â”‚ load_general_plugins()                          â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆ›å»ºæ¨¡å‹æ‰§è¡Œå™¨                                 â”‚       â”‚
â”‚     â”‚ self.model_executor = executor_class(           â”‚       â”‚
â”‚     â”‚     vllm_config                                 â”‚       â”‚
â”‚     â”‚ )                                               â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆå§‹åŒ– KV Cacheï¼ˆå†…å­˜åˆ†é… + æ€§èƒ½åˆ†æï¼‰         â”‚       â”‚
â”‚     â”‚ num_gpu_blocks, num_cpu_blocks, kv_cache_config â”‚       â”‚
â”‚     â”‚     = self._initialize_kv_caches(vllm_config)   â”‚       â”‚
â”‚     â”‚                                                 â”‚       â”‚
â”‚     â”‚ åŒ…æ‹¬ï¼š                                          â”‚       â”‚
â”‚     â”‚ - æ¨¡å‹åŠ è½½åˆ° GPU                                â”‚       â”‚
â”‚     â”‚ - GPU å†…å­˜åˆ†æ                                  â”‚       â”‚
â”‚     â”‚ - åˆ†é… KV cache blocks                          â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆ›å»ºè°ƒåº¦å™¨                                     â”‚       â”‚
â”‚     â”‚ self.scheduler = V1Scheduler(...)               â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ # åˆ›å»ºç»“æ„åŒ–è¾“å‡ºç®¡ç†å™¨                           â”‚       â”‚
â”‚     â”‚ self.structured_output_manager = ...            â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  9. _initialize_kv_caches() (v1/engine/core.py:å†…éƒ¨æ–¹æ³•)        â”‚
â”‚     [åœ¨ EngineCore_DP0 è¿›ç¨‹ä¸­]                                  â”‚
â”‚                                                                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ Step 1: æ¨¡å‹åŠ è½½                                â”‚       â”‚
â”‚     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚       â”‚
â”‚     â”‚ INFO: Starting to load model                   â”‚       â”‚
â”‚     â”‚ INFO: Loading model from scratch...            â”‚       â”‚
â”‚     â”‚ INFO: Using Flash Attention backend on V1      â”‚       â”‚
â”‚     â”‚                                                 â”‚       â”‚
â”‚     â”‚ - åŠ è½½ safetensors æƒé‡æ–‡ä»¶                     â”‚       â”‚
â”‚     â”‚ - 4 ä¸ª shard æ–‡ä»¶é€ä¸ªåŠ è½½                       â”‚       â”‚
â”‚     â”‚ - æ¨¡å‹æƒé‡å ç”¨ï¼š14.9889 GiB                     â”‚       â”‚
â”‚     â”‚ - è€—æ—¶ï¼š3.565280 ç§’                             â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ Step 2: torch.compile ç¼–è¯‘                      â”‚       â”‚
â”‚     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚       â”‚
â”‚     â”‚ INFO: Using cache directory for torch.compile  â”‚       â”‚
â”‚     â”‚ INFO: Dynamo bytecode transform time: 4.43 s   â”‚       â”‚
â”‚     â”‚ INFO: Compiling a graph for dynamic shape      â”‚       â”‚
â”‚     â”‚       takes 18.28 s                             â”‚       â”‚
â”‚     â”‚ INFO: torch.compile takes 22.70 s in total     â”‚       â”‚
â”‚     â”‚                                                 â”‚       â”‚
â”‚     â”‚ - ç¼–è¯‘ level: 3ï¼ˆé»˜è®¤ï¼‰                         â”‚       â”‚
â”‚     â”‚ - åç«¯: inductor                                â”‚       â”‚
â”‚     â”‚ - ç¼“å­˜ç›®å½•: ~/.cache/vllm/torch_compile_cache/  â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ Step 3: æ€§èƒ½åˆ†æä¸ KV Cache åˆ†é…                â”‚       â”‚
â”‚     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚       â”‚
â”‚     â”‚ INFO: Available KV cache memory: 4.95 GiB      â”‚       â”‚
â”‚     â”‚ INFO: GPU KV cache size: 40,528 tokens         â”‚       â”‚
â”‚     â”‚ INFO: Maximum concurrency: 4.05x               â”‚       â”‚
â”‚     â”‚                                                 â”‚       â”‚
â”‚     â”‚ è®¡ç®—é€»è¾‘ï¼š                                      â”‚       â”‚
â”‚     â”‚ - æ€»æ˜¾å­˜ï¼š24 GB (RTX 3090/4090)                â”‚       â”‚
â”‚     â”‚ - æ¨¡å‹å ç”¨ï¼š14.99 GiB                           â”‚       â”‚
â”‚     â”‚ - ç¼–è¯‘ç¼“å­˜ï¼š0.69 GiB                            â”‚       â”‚
â”‚     â”‚ - gpu_memory_utilization: 0.90                 â”‚       â”‚
â”‚     â”‚ - å¯ç”¨ KV cache: 24*0.9 - 14.99 - 0.69 â‰ˆ 4.95 â”‚       â”‚
â”‚     â”‚ - num_gpu_blocks: 2533 (block_size=16)         â”‚       â”‚
â”‚     â”‚ - æ€» tokens: 2533 * 16 = 40,528               â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ Step 4: CUDA Graph æ•è·                         â”‚       â”‚
â”‚     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚       â”‚
â”‚     â”‚ Capturing CUDA graphs (mixed prefill-decode,   â”‚       â”‚
â”‚     â”‚                        PIECEWISE): 67/67        â”‚       â”‚
â”‚     â”‚ Capturing CUDA graphs (decode, FULL): 35/35    â”‚       â”‚
â”‚     â”‚                                                 â”‚       â”‚
â”‚     â”‚ INFO: Graph capturing finished in 6 secs       â”‚       â”‚
â”‚     â”‚ INFO: Graph memory: 0.69 GiB                   â”‚       â”‚
â”‚     â”‚                                                 â”‚       â”‚
â”‚     â”‚ CUDA Graph ç”¨é€”ï¼š                               â”‚       â”‚
â”‚     â”‚ - å‡å°‘ kernel å¯åŠ¨å¼€é”€                          â”‚       â”‚
â”‚     â”‚ - æ•è· 67 ä¸ª prefill-decode æ··åˆåœºæ™¯           â”‚       â”‚
â”‚     â”‚ - æ•è· 35 ä¸ªçº¯ decode åœºæ™¯                      â”‚       â”‚
â”‚     â”‚ - æ”¯æŒçš„ batch sizes: 1-512                    â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚     â”‚ INFO: init engine (profile, create kv cache,   â”‚       â”‚
â”‚     â”‚       warmup model) took 37.12 seconds          â”‚       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  10. run_server_worker() ç»§ç»­ (api_server.py:1907)             â”‚
â”‚      [å›åˆ° APIServer ä¸»è¿›ç¨‹]                                    â”‚
â”‚                                                                 â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # æ„å»º FastAPI åº”ç”¨                             â”‚      â”‚
â”‚      â”‚ app = build_app(args)                           â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # åˆå§‹åŒ–åº”ç”¨çŠ¶æ€                                 â”‚      â”‚
â”‚      â”‚ vllm_config = await engine_client.              â”‚      â”‚
â”‚      â”‚     get_vllm_config()                           â”‚      â”‚
â”‚      â”‚ await init_app_state(                           â”‚      â”‚
â”‚      â”‚     engine_client, vllm_config, app.state, args â”‚      â”‚
â”‚      â”‚ )                                               â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # å¯åŠ¨ HTTP æœåŠ¡å™¨                              â”‚      â”‚
â”‚      â”‚ await serve_http(app, sock=sock, ...)          â”‚      â”‚
â”‚      â”‚                                                 â”‚      â”‚
â”‚      â”‚ INFO: Starting vLLM API server 0 on            â”‚      â”‚
â”‚      â”‚       http://0.0.0.0:6578                       â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  11. build_app() (api_server.py:1511)                          â”‚
â”‚                                                                 â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # åˆ›å»º FastAPI åº”ç”¨                             â”‚      â”‚
â”‚      â”‚ app = FastAPI(lifespan=lifespan)                â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # æ³¨å†Œè·¯ç”±                                       â”‚      â”‚
â”‚      â”‚ app.include_router(router)                      â”‚      â”‚
â”‚      â”‚                                                 â”‚      â”‚
â”‚      â”‚ æ”¯æŒçš„è·¯ç”±ï¼š                                     â”‚      â”‚
â”‚      â”‚ - /v1/chat/completions                          â”‚      â”‚
â”‚      â”‚ - /v1/completions                               â”‚      â”‚
â”‚      â”‚ - /v1/embeddings                                â”‚      â”‚
â”‚      â”‚ - /v1/models                                    â”‚      â”‚
â”‚      â”‚ - /health, /ping                                â”‚      â”‚
â”‚      â”‚ - /tokenize, /detokenize                        â”‚      â”‚
â”‚      â”‚ - /v1/score, /rerank                            â”‚      â”‚
â”‚      â”‚ - ç­‰ 30+ ä¸ªç«¯ç‚¹                                 â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # æ·»åŠ ä¸­é—´ä»¶                                     â”‚      â”‚
â”‚      â”‚ - CORSMiddleware (è·¨åŸŸ)                         â”‚      â”‚
â”‚      â”‚ - AuthenticationMiddleware (API Key)            â”‚      â”‚
â”‚      â”‚ - XRequestIdMiddleware (è¯·æ±‚ ID)                â”‚      â”‚
â”‚      â”‚ - ScalingMiddleware (æ‰©ç¼©å®¹æ£€æŸ¥)                â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # æŒ‚è½½ç›‘æ§                                       â”‚      â”‚
â”‚      â”‚ mount_metrics(app)  # Prometheus /metrics       â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  12. init_app_state() (api_server.py:1606)                     â”‚
â”‚                                                                 â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # è·å–æ”¯æŒçš„ä»»åŠ¡ç±»å‹                             â”‚      â”‚
â”‚      â”‚ supported_tasks = await engine_client.          â”‚      â”‚
â”‚      â”‚     get_supported_tasks()                       â”‚      â”‚
â”‚      â”‚ INFO: Supported_tasks: ['generate']             â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # åŠ è½½ Chat Template                            â”‚      â”‚
â”‚      â”‚ resolved_chat_template = load_chat_template(    â”‚      â”‚
â”‚      â”‚     args.chat_template                          â”‚      â”‚
â”‚      â”‚ )                                               â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # åˆå§‹åŒ–å„ä¸ª Serving ç»„ä»¶                        â”‚      â”‚
â”‚      â”‚                                                 â”‚      â”‚
â”‚      â”‚ state.openai_serving_responses = ...            â”‚      â”‚
â”‚      â”‚ state.openai_serving_chat = OpenAIServingChat(  â”‚      â”‚
â”‚      â”‚     engine_client, model_config, ...            â”‚      â”‚
â”‚      â”‚ )                                               â”‚      â”‚
â”‚      â”‚ state.openai_serving_completion = ...           â”‚      â”‚
â”‚      â”‚ state.openai_serving_pooling = ...              â”‚      â”‚
â”‚      â”‚ state.openai_serving_embedding = ...            â”‚      â”‚
â”‚      â”‚ state.openai_serving_score = ...                â”‚      â”‚
â”‚      â”‚ state.openai_serving_tokenization = ...         â”‚      â”‚
â”‚      â”‚                                                 â”‚      â”‚
â”‚      â”‚ INFO: Using default chat sampling params:      â”‚      â”‚
â”‚      â”‚       {'temperature': 0.6, 'top_p': 0.9}        â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  13. serve_http() (entrypoints/launcher.py)                    â”‚
â”‚                                                                 â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ # å¯åŠ¨ Uvicorn ASGI æœåŠ¡å™¨                      â”‚      â”‚
â”‚      â”‚ config = uvicorn.Config(                        â”‚      â”‚
â”‚      â”‚     app=app,                                    â”‚      â”‚
â”‚      â”‚     host=args.host,                             â”‚      â”‚
â”‚      â”‚     port=args.port,                             â”‚      â”‚
â”‚      â”‚     log_level=args.uvicorn_log_level,           â”‚      â”‚
â”‚      â”‚     timeout_keep_alive=60,                      â”‚      â”‚
â”‚      â”‚     access_log=not args.disable_access_log      â”‚      â”‚
â”‚      â”‚ )                                               â”‚      â”‚
â”‚      â”‚ server = uvicorn.Server(config)                 â”‚      â”‚
â”‚      â”‚ await server.serve()                            â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                          â”‚                                      â”‚
â”‚                          â–¼                                      â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚      â”‚ INFO: Started server process [1883203]          â”‚      â”‚
â”‚      â”‚ INFO: Waiting for application startup.          â”‚      â”‚
â”‚      â”‚ INFO: Application startup complete.             â”‚      â”‚
â”‚      â”‚                                                 â”‚      â”‚
â”‚      â”‚ ğŸ‰ æœåŠ¡å·²å°±ç»ªï¼Œå¯ä»¥æ¥æ”¶è¯·æ±‚                      â”‚      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–

### 1. AsyncLLM (V1 å¼•æ“)

**ä½ç½®**: `vllm/v1/engine/async_llm.py`

**èŒè´£**ï¼š
- å¼‚æ­¥ LLM å¼•æ“çš„ä¸»å…¥å£
- ç®¡ç†è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ
- åè°ƒè¾“å…¥å¤„ç†ã€å¼•æ“æ ¸å¿ƒã€è¾“å‡ºå¤„ç†

**åˆå§‹åŒ–æµç¨‹**ï¼š
```python
AsyncLLM(
    vllm_config=vllm_config,
    executor_class=Executor.get_class(vllm_config),
    log_stats=not disable_log_stats,
    usage_context=UsageContext.OPENAI_API_SERVER,
)
```

**æ ¸å¿ƒç»„ä»¶**ï¼š
- **Processor**: è¾“å…¥é¢„å¤„ç†ï¼ˆTokenizationã€å¤šæ¨¡æ€å¤„ç†ï¼‰
- **OutputProcessor**: è¾“å‡ºåå¤„ç†ï¼ˆDetokenizationã€é‡‡æ ·å‚æ•°åº”ç”¨ï¼‰
- **EngineCoreClient**: å¼•æ“æ ¸å¿ƒå®¢æˆ·ç«¯ï¼ˆå¤šè¿›ç¨‹ IPCï¼‰
- **StatLoggerManager**: ç»Ÿè®¡æ—¥å¿—ç®¡ç†

---

### 2. EngineCore (V1 å¼•æ“æ ¸å¿ƒ)

**ä½ç½®**: `vllm/v1/engine/core.py`

**è¿›ç¨‹**: ç‹¬ç«‹è¿›ç¨‹ `EngineCore_DP0` (Data Parallel rank 0)

**èŒè´£**ï¼š
- æ¨¡å‹åŠ è½½ä¸æ‰§è¡Œ
- KV Cache ç®¡ç†
- è¯·æ±‚è°ƒåº¦
- CUDA Graph ä¼˜åŒ–

**åˆå§‹åŒ–æµç¨‹**ï¼š
```python
EngineCore(
    vllm_config=vllm_config,
    executor_class=executor_class,
    log_stats=log_stats,
)
```

**æ ¸å¿ƒç»„ä»¶**ï¼š
- **ModelExecutor**: æ¨¡å‹æ‰§è¡Œå™¨ï¼ˆGPU æ¨ç†ï¼‰
- **V1Scheduler**: è¯·æ±‚è°ƒåº¦å™¨ï¼ˆChunked Prefill + Continuous Batchingï¼‰
- **KV Cache**: åˆ†é¡µé”®å€¼ç¼“å­˜ï¼ˆPagedAttentionï¼‰
- **StructuredOutputManager**: ç»“æ„åŒ–è¾“å‡ºç®¡ç†ï¼ˆJSON Schema çº¦æŸï¼‰

---

### 3. ModelExecutor (æ¨¡å‹æ‰§è¡Œ)

**ä½ç½®**: `vllm/v1/executor/`

**èŒè´£**ï¼š
- åŠ è½½æ¨¡å‹æƒé‡åˆ° GPU
- æ‰§è¡Œå‰å‘æ¨ç†
- ç®¡ç†åˆ†å¸ƒå¼å¹¶è¡Œï¼ˆTP/PP/DPï¼‰

**å…³é”®æ“ä½œ**ï¼š

#### 3.1 æ¨¡å‹åŠ è½½
```python
# æ—¥å¿—è¾“å‡ºç¤ºä¾‹
INFO: Starting to load model /data/ysh/models/Llama-3.1-8B-Instruct/...
INFO: Loading model from scratch...

Loading safetensors checkpoint shards: 100% | 4/4 [00:03<00:00, 1.31it/s]
INFO: Loading weights took 3.33 seconds
INFO: Model loading took 14.9889 GiB and 3.565280 seconds
```

**åŠ è½½è¿‡ç¨‹**ï¼š
1. è¯»å– `config.json` è§£ææ¨¡å‹æ¶æ„
2. æ ¹æ®æ¶æ„åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆ`LlamaForCausalLM`ï¼‰
3. ä» safetensors æ–‡ä»¶åŠ è½½æƒé‡ï¼ˆ4 ä¸ª shardï¼‰
4. å°†æƒé‡ä¼ è¾“åˆ° GPUï¼ˆFP16/BF16ï¼‰

#### 3.2 torch.compile ç¼–è¯‘
```python
# æ—¥å¿—è¾“å‡ºç¤ºä¾‹
INFO: Using cache directory: ~/.cache/vllm/torch_compile_cache/...
INFO: Dynamo bytecode transform time: 4.43 s
INFO: Compiling a graph for dynamic shape takes 18.28 s
INFO: torch.compile takes 22.70 s in total
```

**ç¼–è¯‘é…ç½®**ï¼š
```python
CompilationConfig(
    level=3,              # æœ€é«˜ä¼˜åŒ–çº§åˆ«
    backend="inductor",   # PyTorch Inductor åç«¯
    use_inductor=True,
    use_cudagraph=True,   # å¯ç”¨ CUDA Graph
)
```

**ä¼˜åŒ–æ•ˆæœ**ï¼š
- èåˆ kernel æ“ä½œ
- å‡å°‘ Python å¼€é”€
- æå‡æ¨ç†ååé‡ 20-40%

#### 3.3 KV Cache åˆ†é…
```python
# æ—¥å¿—è¾“å‡ºç¤ºä¾‹
INFO: Available KV cache memory: 4.95 GiB
INFO: GPU KV cache size: 40,528 tokens
INFO: Maximum concurrency for 10,000 tokens per request: 4.05x
```

**è®¡ç®—å…¬å¼**ï¼š
```python
total_memory = 24 GB  # GPU æ˜¾å­˜ï¼ˆå¦‚ RTX 4090ï¼‰
model_memory = 14.99 GiB  # æ¨¡å‹æƒé‡
compile_memory = 0.69 GiB  # torch.compile + CUDA Graph

kv_cache_memory = total_memory * gpu_memory_utilization - model_memory - compile_memory
                = 24 * 0.9 - 14.99 - 0.69
                = 4.95 GiB

# æ¯ä¸ª token çš„ KV cache å¤§å°ï¼ˆLlama-3.1-8Bï¼‰
bytes_per_token = 2 * num_layers * num_kv_heads * head_dim * dtype_size
                = 2 * 32 * 8 * 128 * 2  # BF16 = 2 bytes
                = 131,072 bytes
                = 128 KB

# Block å¤§å°å’Œæ•°é‡
block_size = 16  # tokens per block
block_memory = 128 KB * 16 = 2 MB per block

num_gpu_blocks = kv_cache_memory / block_memory
               = 4.95 GiB / 2 MB
               = 2,533 blocks

total_tokens = num_gpu_blocks * block_size
             = 2,533 * 16
             = 40,528 tokens

# æœ€å¤§å¹¶å‘æ•°ï¼ˆmax_model_len = 10,000ï¼‰
max_concurrency = total_tokens / max_model_len
                = 40,528 / 10,000
                = 4.05x
```

#### 3.4 CUDA Graph æ•è·
```python
# æ—¥å¿—è¾“å‡ºç¤ºä¾‹
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% | 67/67 [00:03<00:00]
Capturing CUDA graphs (decode, FULL): 100% | 35/35 [00:01<00:00]

INFO: Graph capturing finished in 6 secs, took 0.69 GiB
```

**æ•è·ç­–ç•¥**ï¼š
- **Mixed Prefill-Decode (PIECEWISE)**: 67 ä¸ªåœºæ™¯
  - æ•è·ä¸åŒ batch size çš„æ··åˆåœºæ™¯ï¼ˆprefill + decodeï¼‰
  - ç”¨äºåŠ¨æ€æ‰¹å¤„ç†
  
- **Decode-only (FULL)**: 35 ä¸ªåœºæ™¯
  - æ•è·çº¯ decode çš„ batch sizes: [1, 2, 4, 8, ..., 512]
  - ç”¨äºç”Ÿæˆé˜¶æ®µçš„æ‰¹é‡æ¨ç†

**ä¼˜åŒ–æ•ˆæœ**ï¼š
- å‡å°‘ kernel å¯åŠ¨å¼€é”€ 10-20%
- å›ºå®šå†…å­˜å¸ƒå±€ï¼Œå‡å°‘ç¢ç‰‡
- æ”¯æŒé«˜ååé‡æ¨ç†

---

### 4. V1Scheduler (è°ƒåº¦å™¨)

**ä½ç½®**: `vllm/v1/core/sched/scheduler.py`

**èŒè´£**ï¼š
- è¯·æ±‚é˜Ÿåˆ—ç®¡ç†
- åˆ†å—é¢„å¡«å……ï¼ˆChunked Prefillï¼‰
- è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰
- ä¼˜å…ˆçº§è°ƒåº¦

**å…³é”®ç‰¹æ€§**ï¼š

#### 4.1 Chunked Prefill
```python
# æ—¥å¿—è¾“å‡ºç¤ºä¾‹
INFO: Chunked prefill is enabled with max_num_batched_tokens=2048.
```

**å·¥ä½œåŸç†**ï¼š
- å°†é•¿ prefill è¯·æ±‚åˆ†æˆå¤šä¸ª chunkï¼ˆæ¯ä¸ª 2048 tokensï¼‰
- ä¸ decode è¯·æ±‚æ··åˆæ‰¹å¤„ç†
- é¿å…é•¿è¯·æ±‚é˜»å¡çŸ­è¯·æ±‚

**ç¤ºä¾‹**ï¼š
```
è¯·æ±‚ A: prefill 8192 tokens â†’ åˆ†æˆ 4 ä¸ª chunk (2048 * 4)
è¯·æ±‚ B: decode 128 tokens
è¯·æ±‚ C: prefill 1024 tokens â†’ 1 ä¸ª chunk

Batch 1: [A_chunk_1, B_decode, C_prefill]  # 2048 + 128 + 1024 = 3200 tokens
Batch 2: [A_chunk_2, B_decode]
Batch 3: [A_chunk_3, B_decode]
Batch 4: [A_chunk_4, B_decode]
```

#### 4.2 Continuous Batching
- åŠ¨æ€è°ƒæ•´ batch size
- å®Œæˆçš„è¯·æ±‚ç«‹å³ä» batch ä¸­ç§»é™¤
- æ–°è¯·æ±‚ç«‹å³åŠ å…¥ batchï¼ˆå¦‚æœ‰ç©ºé—´ï¼‰

---

### 5. FastAPI åº”ç”¨

**ä½ç½®**: `vllm/entrypoints/openai/api_server.py`

**èŒè´£**ï¼š
- HTTP API æœåŠ¡
- è¯·æ±‚éªŒè¯ä¸è·¯ç”±
- OpenAI åè®®å…¼å®¹

**è·¯ç”±åˆ—è¡¨** (å…± 30+ ä¸ªç«¯ç‚¹)ï¼š

| è·¯ç”± | æ–¹æ³• | åŠŸèƒ½ |
|------|------|------|
| `/v1/chat/completions` | POST | Chat å¯¹è¯å®Œæˆ |
| `/v1/completions` | POST | æ–‡æœ¬å®Œæˆ |
| `/v1/embeddings` | POST | æ–‡æœ¬åµŒå…¥ |
| `/v1/models` | GET | åˆ—å‡ºå¯ç”¨æ¨¡å‹ |
| `/health` | GET | å¥åº·æ£€æŸ¥ |
| `/ping` | GET/POST | æœåŠ¡å­˜æ´»æ£€æŸ¥ |
| `/tokenize` | POST | Tokenization |
| `/detokenize` | POST | Detokenization |
| `/v1/score` | POST | åºåˆ—è¯„åˆ† |
| `/rerank` | POST | æ–‡æ¡£é‡æ’åº |
| `/v1/audio/transcriptions` | POST | è¯­éŸ³è½¬æ–‡æœ¬ |
| `/metrics` | GET | Prometheus ç›‘æ§æŒ‡æ ‡ |

**ä¸­é—´ä»¶**ï¼š
- **CORSMiddleware**: å¤„ç†è·¨åŸŸè¯·æ±‚
- **AuthenticationMiddleware**: API Key éªŒè¯
- **XRequestIdMiddleware**: è¯·æ±‚ ID è¿½è¸ª
- **ScalingMiddleware**: æ‰©ç¼©å®¹çŠ¶æ€æ£€æŸ¥

---

## æ—¶é—´çº¿åˆ†æ

### å¯åŠ¨æ—¶é—´åˆ†å¸ƒ

| é˜¶æ®µ | è€—æ—¶ | å æ¯” | å…³é”®æ“ä½œ |
|------|------|------|---------|
| **å‚æ•°è§£æ** | 3s | 6% | CLI å‚æ•°è§£æã€ç¯å¢ƒå˜é‡è¯»å– |
| **æ¨¡å‹åŠ è½½** | 6s | 12% | è¯»å– safetensorsã€ä¼ è¾“åˆ° GPU |
| **torch.compile** | 26s | 52% | Dynamo å­—èŠ‚ç è½¬æ¢ã€Inductor ç¼–è¯‘ |
| **KV Cache åˆ†é…** | 1s | 2% | GPU å†…å­˜åˆ†æã€block åˆ†é… |
| **CUDA Graph æ•è·** | 6s | 12% | æ•è· 102 ä¸ªå›¾ï¼ˆ67 prefill + 35 decodeï¼‰ |
| **æœåŠ¡åˆå§‹åŒ–** | 8s | 16% | FastAPI è·¯ç”±æ³¨å†Œã€ç»„ä»¶åˆå§‹åŒ– |
| **æ€»è®¡** | **50s** | **100%** | ä»å¯åŠ¨åˆ°æœåŠ¡å°±ç»ª |

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### 1. å‡å°‘ torch.compile æ—¶é—´
- **ç¼“å­˜å¤ç”¨**ï¼šé¦–æ¬¡å¯åŠ¨åç¼“å­˜åœ¨ `~/.cache/vllm/torch_compile_cache/`
- **åç»­å¯åŠ¨**ï¼šè·³è¿‡ç¼–è¯‘ç›´æ¥åŠ è½½ç¼“å­˜ï¼ˆçº¦ 5sï¼‰
- **ç¦ç”¨ç¼–è¯‘**ï¼ˆå¦‚æœååé‡è¦æ±‚ä¸é«˜ï¼‰ï¼š
  ```bash
  VLLM_TORCH_COMPILE_LEVEL=0 python -m vllm.entrypoints.openai.api_server ...
  ```

#### 2. å‡å°‘æ¨¡å‹åŠ è½½æ—¶é—´
- **ä½¿ç”¨ SSD å­˜å‚¨**ï¼šå‡å°‘ç£ç›˜ I/Oï¼ˆ4s â†’ 2sï¼‰
- **é¢„çƒ­æ–‡ä»¶ç³»ç»Ÿç¼“å­˜**ï¼š
  ```bash
  cat /data/ysh/models/Llama-3.1-8B-Instruct/*.safetensors > /dev/null
  ```

#### 3. å‡å°‘ CUDA Graph æ•è·æ—¶é—´
- **å‡å°‘æ•è·åœºæ™¯**ï¼š
  ```bash
  --compilation-config '{"cudagraph_capture_sizes": [1, 2, 4, 8, 16, 32]}'
  ```
  è€—æ—¶ä» 6s â†’ 2sï¼Œä½†ç‰ºç‰²éƒ¨åˆ†æ€§èƒ½

---

## å…³é”®ä»£ç è·¯å¾„

### æ–‡ä»¶è·¯å¾„æ˜ å°„

| ç»„ä»¶ | æ–‡ä»¶è·¯å¾„ | è¡Œæ•° | åŠŸèƒ½ |
|------|---------|------|------|
| **å‘½ä»¤è¡Œå…¥å£** | `vllm/entrypoints/openai/api_server.py:1942` | 12 | `if __name__ == "__main__"` |
| **æœåŠ¡å™¨å¯åŠ¨** | `vllm/entrypoints/openai/api_server.py:1877` | 63 | `run_server()`, `run_server_worker()` |
| **å¼•æ“åˆ›å»º** | `vllm/entrypoints/openai/api_server.py:152` | 88 | `build_async_engine_client()` |
| **AsyncLLM åˆå§‹åŒ–** | `vllm/v1/engine/async_llm.py:54` | 132 | `AsyncLLM.__init__()` |
| **EngineCore åˆå§‹åŒ–** | `vllm/v1/engine/core.py:62` | 174 | `EngineCore.__init__()` |
| **KV Cache åˆå§‹åŒ–** | `vllm/v1/engine/core.py:å†…éƒ¨` | - | `_initialize_kv_caches()` |
| **æ¨¡å‹æ‰§è¡Œå™¨** | `vllm/v1/executor/gpu_executor.py` | - | `GPUExecutor` |
| **è°ƒåº¦å™¨** | `vllm/v1/core/sched/scheduler.py` | - | `V1Scheduler` |
| **FastAPI åº”ç”¨** | `vllm/entrypoints/openai/api_server.py:1511` | 94 | `build_app()` |
| **åº”ç”¨çŠ¶æ€åˆå§‹åŒ–** | `vllm/entrypoints/openai/api_server.py:1606` | 244 | `init_app_state()` |
| **HTTP æœåŠ¡å¯åŠ¨** | `vllm/entrypoints/launcher.py` | - | `serve_http()` |

### è¿›ç¨‹æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  APIServer (ä¸»è¿›ç¨‹, PID=1883203)                  â”‚
â”‚  - FastAPI åº”ç”¨                                   â”‚
â”‚  - è¯·æ±‚æ¥æ”¶ä¸éªŒè¯                                  â”‚
â”‚  - AsyncLLM (å¼•æ“å®¢æˆ·ç«¯)                          â”‚
â”‚    â”œâ”€ Processor (è¾“å…¥é¢„å¤„ç†)                      â”‚
â”‚    â”œâ”€ OutputProcessor (è¾“å‡ºåå¤„ç†)                â”‚
â”‚    â””â”€ EngineCoreClient (IPC å®¢æˆ·ç«¯)               â”‚
â”‚        â”‚                                          â”‚
â”‚        â”‚ ZMQ Socket IPC                           â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EngineCore_DP0 (å­è¿›ç¨‹, PID=1883375)             â”‚
â”‚  - æ¨¡å‹åŠ è½½ (14.99 GiB)                           â”‚
â”‚  - torch.compile (22.7s)                          â”‚
â”‚  - KV Cache (2533 blocks, 4.95 GiB)              â”‚
â”‚  - CUDA Graph (102 graphs, 0.69 GiB)             â”‚
â”‚  - V1Scheduler (è¯·æ±‚è°ƒåº¦)                         â”‚
â”‚  - ModelExecutor (GPU æ¨ç†)                       â”‚
â”‚    â””â”€ GPU Worker                                  â”‚
â”‚        â”œâ”€ Flash Attention Backend                 â”‚
â”‚        â”œâ”€ PagedAttention                          â”‚
â”‚        â””â”€ CUDA Kernels                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### IPC é€šä¿¡æœºåˆ¶

**é€šä¿¡åè®®**: ZMQ (Zero Message Queue)

**æ¶ˆæ¯ç±»å‹**:
1. **EngineCoreRequest**: æ¨ç†è¯·æ±‚ï¼ˆadd_request, abort_requestï¼‰
2. **EngineCoreOutputs**: æ¨ç†è¾“å‡ºï¼ˆtoken ids, logprobsï¼‰
3. **UtilityRequest**: å·¥å…·è¯·æ±‚ï¼ˆget_model_config, do_log_statsï¼‰
4. **UtilityResult**: å·¥å…·å“åº”

**é€šä¿¡æµç¨‹**:
```python
# APIServer â†’ EngineCore
engine_client.add_request(request_id, prompt, params)
    â†“
[ZMQ Socket] â†’ IPC åºåˆ—åŒ–ï¼ˆmsgpackï¼‰
    â†“
EngineCore æ¥æ”¶è¯·æ±‚ â†’ åŠ å…¥è°ƒåº¦é˜Ÿåˆ—
    â†“
è°ƒåº¦å™¨åˆ†é…èµ„æº â†’ æ‰§è¡Œæ¨ç†
    â†“
ç”Ÿæˆè¾“å‡º â†’ åºåˆ—åŒ–ï¼ˆmsgpackï¼‰
    â†“
[ZMQ Socket] â† IPC ä¼ è¾“
    â†“
engine_client æ¥æ”¶è¾“å‡º â†’ å¼‚æ­¥è¿”å›
```

---

## æ—¥å¿—å…³é”®å­—è§£æ

| æ—¥å¿—å…³é”®å­— | å«ä¹‰ | ä»£ç ä½ç½® |
|-----------|------|---------|
| `vLLM API server version` | æœåŠ¡å™¨ç‰ˆæœ¬ä¿¡æ¯ | `api_server.py:æ‰“å°å¯åŠ¨ä¿¡æ¯` |
| `non-default args` | ç”¨æˆ·æŒ‡å®šçš„å‚æ•° | `utils.py:233` |
| `Resolved architecture` | æ£€æµ‹åˆ°çš„æ¨¡å‹æ¶æ„ | `model.py:547` |
| `Using max model len` | æœ€å¤§åºåˆ—é•¿åº¦ | `model.py:1510` |
| `Chunked prefill is enabled` | åˆ†å—é¢„å¡«å……å¼€å¯ | `scheduler.py:205` |
| `Automatically detected platform` | æ£€æµ‹åˆ°çš„ç¡¬ä»¶å¹³å° | `__init__.py:216` |
| `Initializing a V1 LLM engine` | V1 å¼•æ“åˆå§‹åŒ–å¼€å§‹ | `core.py:77` |
| `Starting to load model` | å¼€å§‹åŠ è½½æ¨¡å‹ | `gpu_model_runner.py:2602` |
| `Using Flash Attention backend` | æ³¨æ„åŠ›åç«¯é€‰æ‹© | `cuda.py:366` |
| `Loading weights took X seconds` | æƒé‡åŠ è½½è€—æ—¶ | `default_loader.py:267` |
| `torch.compile takes X s in total` | ç¼–è¯‘æ€»è€—æ—¶ | `monitor.py:34` |
| `Available KV cache memory` | å¯ç”¨ KV cache å†…å­˜ | `gpu_worker.py:298` |
| `GPU KV cache size: X tokens` | KV cache å®¹é‡ | `kv_cache_utils.py:1087` |
| `Maximum concurrency` | æœ€å¤§å¹¶å‘è¯·æ±‚æ•° | `kv_cache_utils.py:1091` |
| `Capturing CUDA graphs` | CUDA Graph æ•è·è¿›åº¦ | è¿›åº¦æ¡è¾“å‡º |
| `Graph capturing finished` | æ•è·å®Œæˆ | `gpu_model_runner.py:3480` |
| `init engine took X seconds` | å¼•æ“åˆå§‹åŒ–æ€»è€—æ—¶ | `core.py:210` |
| `Supported_tasks` | æ”¯æŒçš„ä»»åŠ¡ç±»å‹ | `api_server.py:1634` |
| `Using default chat sampling params` | é»˜è®¤é‡‡æ ·å‚æ•° | `serving_chat.py:139` |
| `Starting vLLM API server` | æœåŠ¡å™¨å¯åŠ¨ | `api_server.py:1912` |
| `Available routes` | å¯ç”¨çš„ API è·¯ç”± | `launcher.py:34` |
| `Application startup complete` | åº”ç”¨å¯åŠ¨å®Œæˆ | Uvicorn è¾“å‡º |

---

## å¸¸è§é—®é¢˜æ’æŸ¥

### 1. å¯åŠ¨å¤±è´¥ï¼šæ˜¾å­˜ä¸è¶³

**é”™è¯¯æ—¥å¿—**ï¼š
```
torch.cuda.OutOfMemoryError: CUDA out of memory.
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# é™ä½ GPU å†…å­˜ä½¿ç”¨ç‡
--gpu-memory-utilization 0.75  # é»˜è®¤ 0.90

# å‡å°‘æœ€å¤§åºåˆ—é•¿åº¦
--max-model-len 4096  # é»˜è®¤æ¨¡å‹é…ç½®å€¼

# ç¦ç”¨ torch.compileï¼ˆèŠ‚çœ 0.69 GiBï¼‰
VLLM_TORCH_COMPILE_LEVEL=0 python -m vllm.entrypoints.openai.api_server ...

# ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆèŠ‚çœ 50% æ˜¾å­˜ï¼‰
--quantization awq  # æˆ– gptq, fp8
```

### 2. å¯åŠ¨æ…¢ï¼štorch.compile è¶…æ—¶

**ç—‡çŠ¶**ï¼š
```
INFO: Compiling a graph for dynamic shape takes 18.28 s
# é•¿æ—¶é—´åœç•™åœ¨è¿™ä¸€æ­¥
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ–¹æ¡ˆ 1: ç¦ç”¨ torch.compileï¼ˆç‰ºç‰² 20-40% ååé‡ï¼‰
VLLM_TORCH_COMPILE_LEVEL=0

# æ–¹æ¡ˆ 2: ä½¿ç”¨ç¼“å­˜ï¼ˆç¬¬äºŒæ¬¡å¯åŠ¨å¿«é€Ÿï¼‰
# ç¼“å­˜ä½ç½®: ~/.cache/vllm/torch_compile_cache/
# ç¡®ä¿ç¼“å­˜ç›®å½•å¯å†™

# æ–¹æ¡ˆ 3: é™ä½ç¼–è¯‘çº§åˆ«
--compilation-config '{"level": 1}'  # é»˜è®¤ level=3
```

### 3. FlashInfer è­¦å‘Š

**è­¦å‘Šæ—¥å¿—**ï¼š
```
WARNING: FlashInfer is not available. Falling back to PyTorch-native implementation
```

**å½±å“**ï¼šé‡‡æ ·æ€§èƒ½ä¸‹é™ï¼ˆtop-p/top-kï¼‰ï¼Œæ¨ç†ååé‡å½±å“è¾ƒå°

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å®‰è£… FlashInfer
pip install flashinfer -i https://flashinfer.ai/whl/cu124/torch2.4/

# æˆ–å¿½ç•¥è­¦å‘Šï¼ˆä¸å½±å“æ ¸å¿ƒæ¨ç†ï¼‰
```

### 4. å¤šè¿›ç¨‹é€šä¿¡å¤±è´¥

**é”™è¯¯æ—¥å¿—**ï¼š
```
[c10d] The client socket cannot be initialized to connect to [::ffff:...]:40933
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# ç¦ç”¨ IPv6ï¼ˆå¦‚æœä¸éœ€è¦ï¼‰
export VLLM_WORKER_MULTIPROC_METHOD=spawn  # æˆ– fork

# æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
# æ£€æŸ¥ /etc/hosts é…ç½®
```

---

## æ€§èƒ½åŸºå‡†

### æµ‹è¯•ç¯å¢ƒ
- **GPU**: NVIDIA RTX 4090 (24 GB)
- **æ¨¡å‹**: Llama-3.1-8B-Instruct
- **max_model_len**: 10,000
- **gpu_memory_utilization**: 0.90

### èµ„æºå ç”¨

| èµ„æº | å ç”¨é‡ | è¯´æ˜ |
|------|--------|------|
| **GPU æ˜¾å­˜** | 15.68 GiB | æ¨¡å‹ 14.99 + ç¼–è¯‘ 0.69 |
| **KV Cache** | 4.95 GiB | 2533 blocks Ã— 2 MB |
| **æ€»æ˜¾å­˜** | 20.63 GiB | å ç”¨ 85.9% (24 GB æ˜¾å¡) |
| **CPU å†…å­˜** | ~4 GB | FastAPI + AsyncLLM |
| **å¯åŠ¨æ—¶é—´** | 50 ç§’ | é¦–æ¬¡å¯åŠ¨ï¼ˆåŒ…å«ç¼–è¯‘ï¼‰ |
| **å¯åŠ¨æ—¶é—´** | 12 ç§’ | åç»­å¯åŠ¨ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰ |

### æ¨ç†æ€§èƒ½

| æŒ‡æ ‡ | æ•°å€¼ | æµ‹è¯•æ¡ä»¶ |
|------|------|---------|
| **æœ€å¤§å¹¶å‘** | 4.05x | max_model_len=10,000 |
| **ååé‡** | ~2,000 tokens/s | Batch size=32, decode-only |
| **é¦– token å»¶è¿Ÿ** | 30-50 ms | Prefill 512 tokens |
| **Token é—´å»¶è¿Ÿ** | 15-25 ms | Decode phase |

---

## å‚è€ƒä»£ç ä½ç½®

### æ ¸å¿ƒæ–‡ä»¶åˆ—è¡¨

```
vllm/
â”œâ”€â”€ entrypoints/
â”‚   â”œâ”€â”€ openai/
â”‚   â”‚   â”œâ”€â”€ api_server.py           # ä¸»å…¥å£ï¼ŒFastAPI åº”ç”¨
â”‚   â”‚   â”œâ”€â”€ cli_args.py             # å‘½ä»¤è¡Œå‚æ•°å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ serving_chat.py         # Chat å¯¹è¯æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ serving_completion.py   # æ–‡æœ¬å®ŒæˆæœåŠ¡
â”‚   â”‚   â””â”€â”€ serving_*.py            # å…¶ä»–æœåŠ¡ç»„ä»¶
â”‚   â””â”€â”€ launcher.py                 # HTTP æœåŠ¡å¯åŠ¨å™¨
â”œâ”€â”€ v1/
â”‚   â”œâ”€â”€ engine/
â”‚   â”‚   â”œâ”€â”€ async_llm.py            # AsyncLLM å¼•æ“
â”‚   â”‚   â”œâ”€â”€ core.py                 # EngineCore æ ¸å¿ƒé€»è¾‘
â”‚   â”‚   â”œâ”€â”€ core_client.py          # å¼•æ“å®¢æˆ·ç«¯ï¼ˆIPCï¼‰
â”‚   â”‚   â”œâ”€â”€ processor.py            # è¾“å…¥å¤„ç†å™¨
â”‚   â”‚   â””â”€â”€ output_processor.py     # è¾“å‡ºå¤„ç†å™¨
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ sched/
â”‚   â”‚       â””â”€â”€ scheduler.py        # V1 è°ƒåº¦å™¨
â”‚   â””â”€â”€ executor/
â”‚       â”œâ”€â”€ gpu_executor.py         # GPU æ‰§è¡Œå™¨
â”‚       â””â”€â”€ gpu_worker.py           # GPU Worker
â”œâ”€â”€ attention/
â”‚   â””â”€â”€ selector.py                 # æ³¨æ„åŠ›åç«¯é€‰æ‹©
â””â”€â”€ platforms/
    â””â”€â”€ cuda.py                     # CUDA å¹³å°é…ç½®
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æ›´æ–°æ—¥æœŸ**: 2026 å¹´ 2 æœˆ 14 æ—¥  
**é€‚ç”¨ vLLM ç‰ˆæœ¬**: 0.11.0+ (V1 å¼•æ“)  
**ä½œè€…**: vLLM Community
